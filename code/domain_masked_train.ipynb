{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "568216b4-8e74-4579-99fa-1fba68ce40fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import subprocess\n",
    "import sys\n",
    "from utils.environment_specific import is_local_development\n",
    "\n",
    "def install_if_missing(package_name, pip_name=None):\n",
    "    try:\n",
    "        importlib.import_module(package_name)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pip_name or package_name])\n",
    "\n",
    "if not is_local_development():\n",
    "    install_if_missing(\"dotenv\", \"python-dotenv\")\n",
    "    install_if_missing(\"tldextract\")\n",
    "    install_if_missing(\"onnxruntime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5e0885d-b58e-4728-924f-fc9165c62a69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import random\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tldextract\n",
    "\n",
    "from utils.dataset import get_dataset_from_args\n",
    "from utils.experiments import setup_mlflow_client, get_run_name\n",
    "from utils.environment_specific import is_local_development\n",
    "from utils.transformers import TransformerModelManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0d5c50d-48e0-4529-b473-5c2f617b38eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a45a9d87-93fc-4fa3-b455-85e0cfeeffbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--seed\", default=42, type=int, help=\"Random seed\")\n",
    "# Model args\n",
    "parser.add_argument(\"--model_type\", default=\"bert_tiny\", type=str, help=\"Type of BERT model to use. see 'get_model_checkpoint' method\")\n",
    "parser.add_argument(\"--dropout\", default=0, type=float, help=\"Dropout rate on final classification layer\")\n",
    "parser.add_argument(\"--decision_threshold\", default=0.5, type=float, help=\"If probability of a class 1 is higher than this, then the sample is classified as class 1\")\n",
    "parser.add_argument(\"--max_sequence_length\", default=256)\n",
    "\n",
    "# Training params\n",
    "parser.add_argument(\"--bert_learning_rate\", default=3e-5, type=float, help=\"AdamW learning rate for everything in model except final classifaction layer\")\n",
    "parser.add_argument(\"--classifier_learning_rate\", default=2e-3, type=float, help=\"AdamW learning rate for classification layer of model\")\n",
    "parser.add_argument(\"--weight_decay\", default=0.01, type=float, help=\"AdamW weight decay for both parts of the model\")\n",
    "parser.add_argument(\"--batch_size\", default=128, type=int, help=\"Batch size\")\n",
    "parser.add_argument(\"--epochs_max\", default=5, type=int, help=\"Maximum number of epochs. Can stop early, however\")\n",
    "parser.add_argument(\"--patience\", default=3, type=int, help=\"Number of epochs to wait for validation accuracy increase before stopping. If it is set to None, then early stopping is not used\")\n",
    "parser.add_argument(\"--freeze_epochs\", default=1, type=int, help=\"Number of epochs to freeze BERT non-final layers initially.\")\n",
    "parser.add_argument(\"--loss\", default=\"focal\", choices=[\"cross_entropy\", \"focal\"], type=str, help=\"Loss function used\")\n",
    "parser.add_argument(\"--focal_loss_gamma\", default=2, help=\"Pass gama parameter if focal loss is being used. Otherwise has no effect\")\n",
    "parser.add_argument(\"--focal_loss_alpha\", default=0.5, help=\"Pass alpha parameter if focal loss is being used. Otherwise has no effect\")\n",
    "\n",
    "# Dataset args\n",
    "parser.add_argument(\"--dataset_name\", default=\"joined\", choices=[\"private_data\", \"any_public_dataset_name\"])\n",
    "parser.add_argument(\"--train_folds\", default=None, type=str, help=\"Which folds of the dataset should be used for training\")\n",
    "parser.add_argument(\"--eval_folds\", default=[4], type=str, help=\"Which folds of the dataset should be used for evaluation\")\n",
    "parser.add_argument(\"--shorten_to_train\", default=None, help=\"How much should train dataset be shortened (400u - 400 records), (10% - 10 percent of all records)\")\n",
    "parser.add_argument(\"--shorten_to_eval\", default=None, help=\"How much should test or validation set be shortened\")\n",
    "\n",
    "default_args = parser.parse_args([])\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8187efe-3f8e-48ae-a92a-aca7b4dddff9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "loaded = load_dotenv(\".env\")\n",
    "if not loaded:\n",
    "    loaded = load_dotenv(\"../../.env\")\n",
    "assert loaded is True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da4f22f4-b178-41be-84c7-81d629282a00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(default_args.seed)\n",
    "torch.manual_seed(default_args.seed)\n",
    "# in case any standard library uses some random function\n",
    "random.seed(default_args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db2c83fc-9092-49cb-9718-bdf6d83a6121",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- threads, parallelism env, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d489d274-318f-43ce-ad0b-606dfd18f962",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: check if useful\n",
    "# num_threads = os.cpu_count()\n",
    "# torch.set_num_threads(num_threads)\n",
    "# if not \"called\" in locals():\n",
    "#     called = True\n",
    "#     torch.set_num_interop_threads(num_threads)\n",
    "# print(f\"Using {num_threads} CPU threads.\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "os.environ[\"NCCL_SHM_DISABLE\"] = \"1\"\n",
    "if not is_local_development():\n",
    "    print(\"Databricks is running\")\n",
    "    assert torch.cuda.is_available(), \"GPU is not available!\"\n",
    "else:\n",
    "    print(\"Local development is running\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcf3a704-5802-4b4b-8d2e-c77d06242063",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# if you find those version and useless warnings annoying, uncomment\n",
    "# import warnings\n",
    "pd.set_option(\"display.max_colwidth\", 120)\n",
    "pd.set_option(\"display.max_rows\", 800)\n",
    "# warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"_distutils_hack\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e3140cb-a7f8-4a3c-9ecc-c1d78baa0f94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5eb379c4-f1fa-4dfd-a324-776a706244af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if is_local_development():\n",
    "    spark = None\n",
    "    # default_args.shorten_to_train = default_args.shorten_to_train or \"4000u\"\n",
    "    # default_args.shorten_to_eval = default_args.shorten_to_eval or \"2000u\"\n",
    "    default_args.shorten_to_train = None\n",
    "    default_args.shorten_to_eval = None\n",
    "    default_args.shorten_to_train = default_args.shorten_to_train or \"1500u\"\n",
    "    default_args.shorten_to_eval = default_args.shorten_to_eval or \"1000u\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62946e1a-3ddc-4455-b111-14ed826077f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def mask_second_level_domain(url):\n",
    "    ext = tldextract.extract(url)\n",
    "    sld = ext.domain\n",
    "    if not sld:\n",
    "        # nothing to replace\n",
    "        return url\n",
    "    return url.replace(sld, \"[MASK]\", 1)\n",
    "\n",
    "\n",
    "def masking_batch_increaser(batch):\n",
    "    flat = []\n",
    "    for url, label in batch:\n",
    "        flat.append((url, label))\n",
    "        flat.append((mask_second_level_domain(url), label))\n",
    "    random.shuffle(flat)\n",
    "    return flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- code for testing that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import URLDatasetPart\n",
    "\n",
    "base_urls = [f\"http://abc{i}.com/{i}\" for i in range(20)]\n",
    "labels = [i % 2 for i in range(20)]\n",
    "\n",
    "print(masking_batch_increaser(zip(base_urls, labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_train_dataset, default_eval_dataset = get_dataset_from_args(default_args, spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37b26ae2-b2ab-4d1f-a47b-058d73d02865",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d923213-6d43-4ad9-87ba-f7994ff2cc9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- load the model to all available GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9086d81-422f-4983-85fc-d03f9fc369de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "setup_mlflow_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae26465a-11a5-404a-9a5a-cadca7bcc416",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def do_transformer_train_test(\n",
    "    args, train_dataset, eval_dataset, num_workers_train=0, num_workers_eval=0, persistent_workers=False, prefetch_factor=None\n",
    "):\n",
    "    \"\"\"Do training run and evaluate on test set designated for evaluation\n",
    "\n",
    "    Use test set from earlier from the script - everything has the same set\n",
    "    \"\"\"\n",
    "    run_name = get_run_name(\"TRAIN\", args.model_type, args.dataset_name)\n",
    "    transformer_model = TransformerModelManager.from_args(args)\n",
    "    print(f\"[runner] Loading dataset...\")\n",
    "    print(f\"[runner] Run name: {run_name}\")\n",
    "    print(f\"[runner] Args: {args}\")\n",
    "    def collate_fn_combination(batch):\n",
    "        new_batch = masking_batch_increaser(batch)\n",
    "        return transformer_model.prepare_batch(new_batch)\n",
    "    \n",
    "    train_data_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn_combination,\n",
    "        prefetch_factor=prefetch_factor,\n",
    "        persistent_workers=persistent_workers,\n",
    "        num_workers=num_workers_train,\n",
    "    )\n",
    "    eval_data_loader = DataLoader(\n",
    "        eval_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=transformer_model.prepare_batch,\n",
    "        prefetch_factor=prefetch_factor,\n",
    "        persistent_workers=persistent_workers,\n",
    "        num_workers=num_workers_eval,\n",
    "    )\n",
    "    print()\n",
    "    with mlflow.start_run(run_name=run_name) as run:\n",
    "        print(f\"[runner]: Run id {run.info.run_id}\")\n",
    "        mlflow.log_params(vars(args))\n",
    "        # log dataset info without actually storing the datasets\n",
    "        train_dataset.log_to_mlflow(\"train\", args.dataset_name, should_save=False)\n",
    "        eval_dataset.log_to_mlflow(\"eval\", args.dataset_name, should_save=False)\n",
    "        transformer_model.train(train_data_loader=train_data_loader, eval_data_loader=eval_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74d04e6b-6547-41f9-ba9d-4109b8b8fe3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "args = copy.deepcopy(default_args)\n",
    "\n",
    "\n",
    "args.model_type=\"bert_mini\"\n",
    "args.dropout = None\n",
    "args.bert_learning_rate=1e-5\n",
    "args.classifier_learning_rate=1e-5\n",
    "args.weight_decay=0\n",
    "args.max_sequence_length=256\n",
    "# will become 2 times bigger at runtime\n",
    "args.batch_size=64\n",
    "args.epochs_max=4\n",
    "args.freeze_epochs=0\n",
    "args.focal_loss_alpha=0.7\n",
    "args.focal_loss_gamma=2\n",
    "args.loss=\"focal\"\n",
    "\n",
    "do_transformer_train_test(args, train_dataset=default_train_dataset, eval_dataset=default_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc066d53-57f1-437d-ab25-58e59bd37b2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# args = copy.deepcopy(default_args)\n",
    "\n",
    "# args.model_type=\"bert_small\"\n",
    "# args.dropout = None\n",
    "# args.bert_learning_rate=1e-5\n",
    "# args.classifier_learning_rate=1e-5\n",
    "# args.weight_decay=0\n",
    "# args.max_sequence_length=256\n",
    "# args.batch_size=128\n",
    "# args.epochs_max=3\n",
    "# args.freeze_epochs=1\n",
    "# args.focal_loss_alpha=-1\n",
    "# args.loss=\"focal\"\n",
    "\n",
    "# do_transformer_train_test(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00dc2f91-44ab-4503-a06a-197b6d8f3589",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# args = copy.deepcopy(default_args)\n",
    "# args.model_type = \"bert_tiny\"\n",
    "# args.loss = \"cross_entropy\"\n",
    "# args.epochs_max = 5\n",
    "# do_transformer_train_test(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "279a3aa9-c5ec-47a6-9af5-56212f001a04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# args = copy.deepcopy(default_args)\n",
    "# args.model_type = \"bert_tiny\"\n",
    "# args.epochs_max = 5\n",
    "# do_transformer_train_test(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e71832b-97fa-473d-ad66-5cc6341d26cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# args = copy.deepcopy(default_args)\n",
    "# args.model_type = \"bert_small\"\n",
    "# do_transformer_train_test(args)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "train_transformer",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "bp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
