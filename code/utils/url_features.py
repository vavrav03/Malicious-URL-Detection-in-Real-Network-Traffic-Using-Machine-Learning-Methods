# This entire file has been generated by ChatGPT O1 (as cited in the thesis), this is not my work, I have only checked for correctness and made some adjustment

from __future__ import annotations
from collections import Counter
from urllib.parse import urlparse, parse_qs, unquote

import math, re, zlib, tldextract
import numpy as np
import pandas as pd


# ---------------------------------------------------------------------
# Low-level helpers (unchanged)
# ---------------------------------------------------------------------
def entropy(text: str) -> float:
    if not text:
        return 0.0
    freqs = Counter(text)
    n = len(text)
    return -sum((c / n) * math.log2(c / n) for c in freqs.values())


def gini_coefficient(x):
    x = np.array(x)
    if np.sum(x) == 0:
        return 0
    sorted_x = np.sort(x)
    n = len(x)
    index = np.arange(1, n + 1)
    return (np.sum((2 * index - n - 1) * sorted_x)) / (n * np.sum(sorted_x))


def kolmogorov_complexity(text: str) -> float:
    return 0.0 if not text else len(zlib.compress(text.encode())) / len(text)


# ---------------------------------------------------------------------
# 1 – Global URL features (Series → DataFrame)
# ---------------------------------------------------------------------
_PUNCTS = {";": "semicolons", "_": "underscores", "?": "question_marks", "=": "equals", "&": "ampersands"}


def url_global_features_vec(urls: pd.Series) -> pd.DataFrame:
    """Vectorized global-URL features."""
    u = urls.fillna("").str.strip()

    df = pd.DataFrame(index=u.index)
    for ch, name in _PUNCTS.items():
        df[f"num_{name}"] = u.str.count(re.escape(ch))

    letters = u.str.count(r"[A-Za-z]")
    digits = u.str.count(r"\d")

    df["length"] = u.str.len()
    df["num_digits"] = u.str.count(r"\d")
    df["non_alnum_cnt"] = u.str.count(r"[^0-9A-Za-z]")
    df["hyphen_cnt"] = u.str.count("-")
    df["at_cnt"] = u.str.count("@")
    df["digit_letter_ratio"] = digits.div(letters.where(letters > 0, np.nan)).fillna(0)
    df["urlentropy"] = u.apply(entropy)
    df["urlkolmogorov_complexity_cplx"] = u.apply(kolmogorov_complexity)
    return df


# ---------------------------------------------------------------------
# 2 – Second-level-domain (SLD) features
# ---------------------------------------------------------------------

def get_sld(url):
    return tldextract.extract(url).domain or ""

def has_ip_address(url):
    ip4_pattern = r"(?:(?:[0-9]){1,3}\.){3}(?:[0-9]){1,3}"
    ip6_pattern = r"(?:(?:[0-9A-Fa-f]{1,4})(?:::|:)){1,7}(?:[0-9A-Fa-f]{1,4}|:)"
    ip_pattern = f"{ip4_pattern}|{ip6_pattern}"
    return bool(re.search(ip_pattern, url))


def sld_features_vec(urls: pd.Series) -> pd.DataFrame:
    slds = urls.apply(lambda x: get_sld(x) or "")

    df = pd.DataFrame(index=urls.index)
    df["sld_contains_ip"] = urls.apply(has_ip_address)
    df["sld_length"] = slds.str.len()
    df["sld_num_digits"] = slds.str.count(r"\d")
    df["sld_non_alnum_cnt"] = slds.str.count(r"[^0-9A-Za-z]")
    df["sld_hyphen_cnt"] = slds.str.count("-")
    df["sld_at_cnt"] = slds.str.count("@")
    return df


# ---------------------------------------------------------------------
# 3 – TLD blacklist
# ---------------------------------------------------------------------
_SUSPICIOUS_TLDS = {
    "click",
    "work",
    "top",
    "xyz",
    "zip",
    "kim",
    "gq",
    "ml",
    "cf",
    "tk",
    "link",
    "men",
    "trade",
    "party",
    "country",
}


def tld_features_vec(urls: pd.Series) -> pd.DataFrame:
    suffixes = urls.fillna("").apply(lambda u: tldextract.extract(u).suffix.lower())
    return pd.DataFrame(
        {"tld_is_suspicious": suffixes.isin(_SUSPICIOUS_TLDS)},
        index=urls.index,
    )


# ---------------------------------------------------------------------
# 4 – Sub-domain features
# ---------------------------------------------------------------------
def subdomain_features_vec(urls: pd.Series) -> pd.DataFrame:
    subs = urls.fillna("").apply(lambda u: tldextract.extract(u).subdomain)

    df = pd.DataFrame(index=urls.index)
    df["subdomain_dot_cnt"] = subs.str.count(r"\.")
    # empty string → 0 subdomains
    df["num_subdomains"] = subs.str.split(".").apply(lambda lst: 0 if lst == [""] else len(lst))
    return df


# ---------------------------------------------------------------------
# 5 – Path features
# ---------------------------------------------------------------------
_SPECIAL_PATH_CHARS = set("!$&'()*+,;=:@%" + "-._~")


def path_features_vec(urls: pd.Series) -> pd.DataFrame:
    parsed = urls.fillna("").apply(urlparse)
    raw_path = parsed.apply(lambda p: p.path)  # still %-escaped
    path = raw_path.apply(unquote)  # decoded for char counts
    dirs = path.str.split("/")

    upper_dirs = dirs.apply(lambda segs: any(s and s[0].isupper() for s in segs))
    single_char = dirs.apply(lambda segs: any(len(s) == 1 for s in segs))
    upper_cnt = path.str.count(r"[A-Z]")
    lower_cnt = path.str.count(r"[a-z]")

    df = pd.DataFrame(index=urls.index)
    df["path_double_slash_cnt"] = path.str.count("//")
    df["path_subdir_cnt"] = dirs.apply(lambda segs: len([s for s in segs if s]))
    df["path_contains_%20"] = raw_path.str.lower().str.contains("%20")
    df["path_has_upper_dirs"] = upper_dirs
    df["path_has_singlechar_dir"] = single_char
    df["path_special_char_cnt"] = path.apply(lambda p: sum(ch in _SPECIAL_PATH_CHARS for ch in p))
    df["path_zero_cnt"] = path.str.count("0")
    df["path_upper_lower_ratio"] = upper_cnt.div(lower_cnt.where(lower_cnt > 0, np.nan)).fillna(0)
    return df


# ---------------------------------------------------------------------
# 6 – Query-string features
# ---------------------------------------------------------------------
def query_features_vec(urls: pd.Series) -> pd.DataFrame:
    parsed = urls.fillna("").apply(urlparse)
    qs_raw = parsed.apply(lambda p: p.query)

    df = pd.DataFrame(index=urls.index)
    df["query_total_len"] = qs_raw.str.len()
    df["query_cnt"] = qs_raw.str.count("&") + qs_raw.ne("").astype(int)
    df["query_var_cnt"] = qs_raw.apply(lambda q: len(parse_qs(q, keep_blank_values=True)))
    return df


# ---------------------------------------------------------------------
# 7 – Keyword flags
# ---------------------------------------------------------------------
_KEYWORDS = ["login", "secure", "cmd=", "redirect=", "confirm", "update", "verify", "account", "bank", "challenge"]


def keyword_features_vec(urls: pd.Series, kw_list=_KEYWORDS) -> pd.DataFrame:
    lo = urls.fillna("").str.lower()
    data = {f"kw_{k.strip('=')}": lo.str.contains(re.escape(k), regex=True) for k in kw_list}
    return pd.DataFrame(data, index=urls.index)


# ---------------------------------------------------------------------
# Convenience wrapper – run every extractor at once
# ---------------------------------------------------------------------
_EXTRACTORS = [
    url_global_features_vec,
    sld_features_vec,
    tld_features_vec,
    subdomain_features_vec,
    path_features_vec,
    query_features_vec,
    keyword_features_vec,
]


def extract_all_vec(urls: pd.Series | list[str]) -> pd.DataFrame:
    """Return a DataFrame with one row per URL and all feature columns. The function is state-less per row - can be run on train and eval set at the same time"""
    urls = pd.Series(urls)  # accept raw list/array as well
    dfs = [fn(urls) for fn in _EXTRACTORS]
    return pd.concat(dfs, axis=1)
