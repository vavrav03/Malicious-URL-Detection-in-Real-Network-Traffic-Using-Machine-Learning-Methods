{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222470f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import subprocess\n",
    "import sys\n",
    "from utils.environment_specific import is_local_development\n",
    "\n",
    "def install_if_missing(package_name, pip_name=None):\n",
    "    try:\n",
    "        importlib.import_module(package_name)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pip_name or package_name])\n",
    "\n",
    "if not is_local_development():\n",
    "    install_if_missing(\"dotenv\", \"python-dotenv\")\n",
    "    install_if_missing(\"onnxruntime\", \"onnxruntime-gpu==1.17.0\")\n",
    "    install_if_missing(\"tldextract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1ef5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import mlflow \n",
    "\n",
    "from utils.dataset import load_public_dataset, load_full_private_df, split_df_by_folds\n",
    "from utils.base_models import find_decision_threshold_maximizing_f1\n",
    "from utils.output import print_dict_level1_inline\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecfdd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_STATE)\n",
    "# in case any standard library uses some random function\n",
    "random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c31ea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = load_dotenv(\".env\")\n",
    "if not loaded:\n",
    "    loaded = load_dotenv(\"../../.env\")\n",
    "assert loaded is True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84949e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_local_development():\n",
    "    spark = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582d2851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(name):\n",
    "    if name != \"private_data\":\n",
    "        df_train, df_test = load_public_dataset(name)\n",
    "    else:\n",
    "        df = load_full_private_df(spark)\n",
    "        df_train, df_test = split_df_by_folds(\n",
    "            df,\n",
    "            train_folds=None,\n",
    "            eval_folds=[4],\n",
    "            shorten_string_train=None,\n",
    "            shorten_string_eval=None,\n",
    "            seed=42,\n",
    "        )\n",
    "    return {\"train\": df_train, \"test\": df_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0464c5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"grambeddings\": get_dataset(\"grambeddings\"),\n",
    "    \"kaggle_binary\": get_dataset(\"kaggle_binary\"),\n",
    "    \"kaggle_multiple\": get_dataset(\"kaggle_multiple\"),\n",
    "    \"mendeley\": get_dataset(\"mendeley\"),\n",
    "    \"joined\": get_dataset(\"joined\"),\n",
    "    # \"private_data\": get_dataset(\"private_data\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dad9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from baselines_code.URLNet.train import get_default_train_args, run_training\n",
    "from baselines_code.URLNet.test import get_default_test_args\n",
    "from utils.base_models import calculate_metrics_binary, log_persistent_performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc8ebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = mlflow.MlflowClient()\n",
    "experiments_folder_path = os.getenv(\"EXPERIMENTS_PATH\")\n",
    "if is_local_development():\n",
    "    experiment_name = \"feature_models\"\n",
    "    print()\n",
    "else:\n",
    "    experiment_name = os.getenv(\"EXPERIMENT_NAME\")\n",
    "\n",
    "experiment_path = os.path.join(experiments_folder_path, experiment_name)\n",
    "mlflow.set_experiment(experiment_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f6455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(train_df, test_df, dataset_name):\n",
    "\n",
    "    train_args = get_default_train_args()\n",
    "    emb_mode = 5\n",
    "    cwd = os.getcwd()\n",
    "    output_dir = os.path.join(cwd, \"baselines\", \"URLNet\", \"runs\", \"temp\") + os.sep\n",
    "\n",
    "    train_args.model_emb_mode = emb_mode\n",
    "    # train_args.log_output_dir = output_dir\n",
    "    test_args = get_default_test_args()\n",
    "    test_args.model_emb_mode = emb_mode\n",
    "    result = run_training(df_train=train_df, df_test=test_df, train_args=train_args, test_args=test_args)\n",
    "\n",
    "    # test_args.data_word_dict_dir = output_dir + \"words_dict.p\"\n",
    "    # test_args.data_subword_dict_dir = output_dir + \"subwords_dict.p\"\n",
    "    # test_args.data_char_dict_dir = output_dir + \"chars_dict.p\"\n",
    "    # test_args.log_checkpoint_dir = output_dir + \"checkpoints/\"\n",
    "    # train_args.log_output_dir = output_dir\n",
    "    # result = run_test(test_df, test_args)\n",
    "\n",
    "    y_true = result[\"targets\"]\n",
    "    y_pred = result[\"predictions\"]\n",
    "    y_probs = result[\"probabilities\"]\n",
    "\n",
    "    # Assuming y_probs is an array of shape (n_samples, 2) for binary classification\n",
    "    unique, counts = np.unique(y_pred, return_counts=True)\n",
    "    print(f\"pred counts: {counts}\")\n",
    "    print(f\"probs: {y_probs[:5]}\")\n",
    "\n",
    "    class1_p = y_probs[:, 1]\n",
    "    metrics = calculate_metrics_binary(y_true, y_probs, y_pred)\n",
    "    metrics[\"n_per_s_total\"] = result[\"n_per_s_total\"]\n",
    "    metrics[\"n_per_s_batched\"] = result[\"n_per_s_batched\"]\n",
    "    metrics[\"params_count\"] = result[\"params_count\"]\n",
    "    best_thr = find_decision_threshold_maximizing_f1(class1_p, y_true)\n",
    "    metrics[\"best_decision_threshold\"] = best_thr\n",
    "    alt_metrics = calculate_metrics_binary(y_true, y_probs, (class1_p >= best_thr).astype(int))\n",
    "    print_dict_level1_inline(metrics)\n",
    "\n",
    "    prefix_path = f\"{dataset_name}/\"\n",
    "    log_persistent_performance(\n",
    "        metrics=metrics,\n",
    "        best_threshold_metrics=alt_metrics,\n",
    "        true_labels=y_true,\n",
    "        class_probabilities=y_probs,\n",
    "        predictions=y_pred,\n",
    "        prefix=prefix_path,\n",
    "        # store_predictions=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c168118",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"URLNet\") as run:\n",
    "    print(run.info.run_id)\n",
    "    for dataset_name in datasets.keys():\n",
    "        run_pipeline(datasets[dataset_name][\"train\"], datasets[dataset_name][\"test\"], dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351e7e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364d49b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bp-env-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
