{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674fc4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import subprocess\n",
    "import sys\n",
    "from utils.environment_specific import is_local_development\n",
    "\n",
    "def install_if_missing(package_name, pip_name=None):\n",
    "    try:\n",
    "        importlib.import_module(package_name)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pip_name or package_name])\n",
    "\n",
    "if not is_local_development():\n",
    "    install_if_missing(\"dotenv\", \"python-dotenv\")\n",
    "    install_if_missing(\"onnx\", \"onnx==1.16.2\")\n",
    "    install_if_missing(\"onnxruntime\", \"onnxruntime-gpu==1.17.0\")\n",
    "\n",
    "# must not be installed together with onnxruntime-transformers (it does not detect GPU then on databriks)\n",
    "# %pip install --upgrade --force-reinstall --ignore-installed onnxruntime-gpu==1.17.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583a85cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import mlflow\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "from utils.dataset import URLDatasetPart, shorten_df_in_smart_way, get_df_by_folds_from_args\n",
    "from utils.environment_specific import is_local_development\n",
    "from utils.transformers import OnnxTransformerModelManager\n",
    "from utils.experiments import setup_mlflow_client, log_dict_mlflow, get_file_size_mb, load_model_from_run\n",
    "from utils.base_models import log_persistent_performance, OnnxModelManager\n",
    "from utils.output import print_dict_level1_inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b34ba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = load_dotenv(\".env\")\n",
    "if not loaded:\n",
    "    loaded = load_dotenv(\"../../.env\")\n",
    "assert loaded is True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7d7646",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "os.environ[\"NCCL_SHM_DISABLE\"] = \"1\"\n",
    "if is_local_development():\n",
    "    spark = None\n",
    "else:\n",
    "    print(\"Local development is running\")\n",
    "    print(\"Databricks is running\")\n",
    "    assert torch.cuda.is_available(), \"GPU is not available!\"\n",
    "    assert \"CUDAExecutionProvider\" in ort.get_available_providers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331ebcd3",
   "metadata": {},
   "source": [
    "- threads, parallelism env, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969b48f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check if useful\n",
    "# num_threads = os.cpu_count()\n",
    "# torch.set_num_threads(num_threads)\n",
    "# if not \"called\" in locals():\n",
    "#     called = True\n",
    "#     torch.set_num_interop_threads(num_threads)\n",
    "# print(f\"Using {num_threads} CPU threads.\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "os.environ[\"NCCL_SHM_DISABLE\"] = \"1\"\n",
    "if not is_local_development():\n",
    "    print(\"Databricks is running\")\n",
    "    assert torch.cuda.is_available(), \"GPU is not available!\"\n",
    "else:\n",
    "    print(\"Local development is running\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fbaaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you find those version and useless warnings annoying, uncomment\n",
    "# import warnings\n",
    "pd.set_option(\"display.max_colwidth\", 120)\n",
    "pd.set_option(\"display.max_rows\", 800)\n",
    "# warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"_distutils_hack\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3006d8c7",
   "metadata": {},
   "source": [
    "# Dataset load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b50ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIGS = {\n",
    "    \"bert_tiny\": {\n",
    "        \"num_attention_heads\": 2,\n",
    "        \"encoder_layers\": 2,\n",
    "        \"hidden_size\": 128,\n",
    "        \"enough_samples_cpu\": 10000,\n",
    "        \"enough_samples_gpu\": 60000,\n",
    "    },\n",
    "    \"bert_mini\": {\n",
    "        \"num_attention_heads\": 4,\n",
    "        \"encoder_layers\": 4,\n",
    "        \"hidden_size\": 256,\n",
    "        \"enough_samples_cpu\": 5000,\n",
    "        \"enough_samples_gpu\": 30000,\n",
    "    },\n",
    "    \"bert_small\": {\n",
    "        \"num_attention_heads\": 8,\n",
    "        \"encoder_layers\": 4,\n",
    "        \"hidden_size\": 512,\n",
    "        \"enough_samples_cpu\": 1000,\n",
    "        \"enough_samples_gpu\": 10000,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff9dc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_mlflow_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb72b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = \"./working_dir\"\n",
    "global_tokenizer = None\n",
    "global_args = None\n",
    "global_torch_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373760be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size_stats(session):\n",
    "    path = session._model_path\n",
    "    size_mb = get_file_size_mb(path)\n",
    "\n",
    "    model = onnx.load(path)\n",
    "    total_params = sum(int(np.prod(init.dims)) for init in model.graph.initializer)\n",
    "\n",
    "    return {\n",
    "        \"onnx_model_size_mb\": round(size_mb, 2),\n",
    "        \"total_parameters\": total_params,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_enough_samples_perf(session):\n",
    "    providers = session.get_providers()\n",
    "    if \"CUDAExecutionProvider\" in providers:\n",
    "        return MODEL_CONFIGS[global_args.model_type]['enough_samples_gpu']\n",
    "    else:\n",
    "        return MODEL_CONFIGS[global_args.model_type]['enough_samples_cpu']\n",
    "\n",
    "\n",
    "def evaluate_metrics_and_performance(session: OnnxModelManager, dataset, output_folder: str, params_to_log, measure_performance=False, evaluate=False, log_model_stats=True, log_model=False):\n",
    "    evaluated_model = OnnxTransformerModelManager(session, global_tokenizer, global_args)\n",
    "    metrics, perf = None, None\n",
    "    dl = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=evaluated_model.args.batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=evaluated_model.prepare_batch,\n",
    "    )\n",
    "    if measure_performance:\n",
    "        perf = evaluated_model.measure_performance(\n",
    "            data_loader=dl, enough_samples_to_measure=get_enough_samples_perf(session), log_to_mlflow=False\n",
    "        )\n",
    "        print_dict_level1_inline(perf)\n",
    "        log_dict_mlflow(perf, f\"{output_folder}/performance.json\")\n",
    "\n",
    "    if log_model_stats:\n",
    "        model_size_stats = get_model_size_stats(session)\n",
    "        print(model_size_stats)\n",
    "        log_dict_mlflow(model_size_stats, f\"{output_folder}/model_size_stats.json\")\n",
    "    if log_model:\n",
    "        mlflow.log_artifact(session._model_path, artifact_path=output_folder)\n",
    "\n",
    "    if evaluate:\n",
    "        metrics, output, best_threshold_metrics = evaluated_model.evaluate(data_loader=dl)\n",
    "        log_persistent_performance(\n",
    "            metrics=metrics,\n",
    "            best_threshold_metrics=best_threshold_metrics,\n",
    "            true_labels=output[\"true_labels\"],\n",
    "            class_probabilities=output[\"class_probabilities\"],\n",
    "            predictions=output[\"predictions\"],\n",
    "            prefix=f\"{output_folder}/\",\n",
    "            # store_predictions=True,\n",
    "        )\n",
    "        print_dict_level1_inline(metrics)\n",
    "    if params_to_log is not None:\n",
    "        log_dict_mlflow(params_to_log, f\"{output_folder}/config.json\")\n",
    "\n",
    "    return metrics, perf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a61514",
   "metadata": {},
   "source": [
    "# ONNX Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e14cd6",
   "metadata": {},
   "source": [
    "- Purpose of this part:\n",
    "    - Get baseline metrics (GPU) and performance (CPU) scores to compare with dynamic quantization on limited dataset\n",
    "    - Get GPU performance to check that it is similar to Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a54884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_baseline_evaluation(onnx_baseline_model_path, cpu_eval_dataset, full_eval_dataset):\n",
    "    # CPU performance\n",
    "    session = OnnxTransformerModelManager.get_cpu_session(onnx_baseline_model_path)\n",
    "    baseline_cpu_perf, _ = evaluate_metrics_and_performance(session, cpu_eval_dataset, output_folder=\"onnx_baseline_cpu\", params_to_log=None, measure_performance=True)\n",
    "    # we want evaluation comparison on CPU dataset - but it can be run with GPU to do it faster. Results are stored inside same folder.\n",
    "    session = OnnxTransformerModelManager.get_gpu_session(onnx_baseline_model_path)\n",
    "    baseline_cpu_metrics, _ = evaluate_metrics_and_performance(session, cpu_eval_dataset, output_folder=\"onnx_baseline_cpu\", params_to_log=None, evaluate=True, log_model_stats=False)\n",
    "\n",
    "    # Eval GPU performance\n",
    "    session = OnnxTransformerModelManager.get_gpu_session(onnx_baseline_model_path)\n",
    "    _, baseline_gpu_perf = evaluate_metrics_and_performance(session, full_eval_dataset, output_folder=\"onnx_baseline_gpu\", params_to_log=None, measure_performance=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd717f0",
   "metadata": {},
   "source": [
    "# Optimized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818e9376",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.transformers.optimizer import optimize_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3da4364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_optimized_model_part(baseline_model_path, full_eval_dataset):\n",
    "    model_name = \"onnx_baseline_optimized\"\n",
    "    optimized_fp32_path = os.path.join(working_dir, f\"{model_name}.onnx\")\n",
    "    optimized_model = optimize_model(\n",
    "        baseline_model_path, model_type=\"bert\", use_gpu=True, opt_level=2, num_heads=MODEL_CONFIGS[global_args.model_type]['num_attention_heads'], hidden_size=MODEL_CONFIGS[global_args.model_type]['hidden_size']\n",
    "    )\n",
    "    optimized_model.save_model_to_file(optimized_fp32_path)\n",
    "    session_fp32_opt = OnnxTransformerModelManager.get_gpu_session(optimized_fp32_path)\n",
    "    optimized_metrics, optimized_perf = evaluate_metrics_and_performance(\n",
    "        session_fp32_opt, full_eval_dataset, model_name, params_to_log=None, measure_performance=True, evaluate=True, log_model=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ba234b",
   "metadata": {},
   "source": [
    "# Float16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b710be3",
   "metadata": {},
   "source": [
    "- onnxruntime optimizer variant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d81f99",
   "metadata": {},
   "source": [
    "## Without optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a04adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_float16_part_without_optimizations(baseline_model_path, model_type, full_eval_dataset):\n",
    "    model_name = \"onnx_baseline_opt_fp16_without_optimizations\"\n",
    "    onnx_fp16_path = os.path.join(working_dir, f\"{model_name}.onnx\")\n",
    "\n",
    "    opt_model = optimize_model(\n",
    "        baseline_model_path,\n",
    "        model_type=\"bert\",\n",
    "        use_gpu=True,\n",
    "        opt_level=0,\n",
    "        num_heads=MODEL_CONFIGS[model_type][\"num_attention_heads\"],\n",
    "        hidden_size=MODEL_CONFIGS[model_type][\"hidden_size\"],\n",
    "    )\n",
    "    opt_model.convert_float_to_float16(keep_io_types=True)\n",
    "    opt_model.save_model_to_file(onnx_fp16_path)\n",
    "    session = OnnxTransformerModelManager.get_gpu_session(onnx_fp16_path)\n",
    "    gpu_fp16_metrics, gpu_fp16_perf = evaluate_metrics_and_performance(\n",
    "        session=session, dataset=full_eval_dataset, output_folder=model_name, params_to_log=None, measure_performance=True, evaluate=True, log_model=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6a44e3",
   "metadata": {},
   "source": [
    "## With optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be01f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_float16_part(baseline_model_path, model_type, full_eval_dataset):\n",
    "    model_name = \"onnx_baseline_opt_fp16\"\n",
    "    onnx_fp16_path = os.path.join(working_dir, f\"{model_name}.onnx\")\n",
    "\n",
    "    opt_model = optimize_model(\n",
    "        baseline_model_path,\n",
    "        model_type=\"bert\",\n",
    "        use_gpu=True,\n",
    "        opt_level=2,\n",
    "        num_heads=MODEL_CONFIGS[model_type][\"num_attention_heads\"],\n",
    "        hidden_size=MODEL_CONFIGS[model_type][\"hidden_size\"],\n",
    "    )\n",
    "    opt_model.convert_float_to_float16(keep_io_types=True)\n",
    "    opt_model.save_model_to_file(onnx_fp16_path)\n",
    "    session = OnnxTransformerModelManager.get_gpu_session(onnx_fp16_path)\n",
    "    gpu_fp16_metrics, gpu_fp16_perf = evaluate_metrics_and_performance(\n",
    "        session=session, dataset=full_eval_dataset, output_folder=model_name, params_to_log=None, measure_performance=True, evaluate=True, log_model=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c8a36b",
   "metadata": {},
   "source": [
    "# Dynamic quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f299d1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "def do_dynamic_quant(onnx_baseline_model_path, cpu_eval_dataset):\n",
    "    model_name = \"onnx_baseline_dynq_int8\"\n",
    "    dynq_model_path = os.path.join(working_dir, f\"{model_name}.onnx\")\n",
    "\n",
    "    quant_config = {\n",
    "        \"weight_type\": QuantType.QInt8,\n",
    "        \"op_types_to_quantize\": [\"MatMul\", \"Gemm\"],\n",
    "        \"reduce_range\": True,\n",
    "        \"per_channel\": True,\n",
    "    }\n",
    "\n",
    "    quantize_dynamic(\n",
    "        model_input=onnx_baseline_model_path,\n",
    "        model_output=dynq_model_path,\n",
    "        weight_type=quant_config[\"weight_type\"],\n",
    "        op_types_to_quantize=quant_config[\"op_types_to_quantize\"],\n",
    "        reduce_range=quant_config[\"reduce_range\"],\n",
    "        per_channel=quant_config[\"per_channel\"],\n",
    "    )\n",
    "\n",
    "    session_cpu = OnnxTransformerModelManager.get_cpu_session(dynq_model_path)\n",
    "    dynq_metrics, dynq_perf = evaluate_metrics_and_performance(\n",
    "        session=session_cpu, dataset=cpu_eval_dataset, output_folder=model_name, params_to_log=quant_config, measure_performance=True, evaluate=True, log_model=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235959d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_run(spark, cpu_dataset_max_length=16000):\n",
    "    model_type = global_args.model_type\n",
    "    np.random.seed(global_args.seed)\n",
    "    torch.manual_seed(global_args.seed)\n",
    "    # in case any standard library uses some random function\n",
    "    random.seed(global_args.seed)\n",
    "\n",
    "    if is_local_development():\n",
    "        global_args.shorten_to_train = None\n",
    "        global_args.shorten_to_eval = None\n",
    "        global_args.shorten_to_train = global_args.shorten_to_train or \"10000u\"\n",
    "        global_args.shorten_to_eval = global_args.shorten_to_eval or \"1000u\"\n",
    "    full_train_df, full_eval_df = get_df_by_folds_from_args(global_args, spark)\n",
    "\n",
    "    full_train_dataset = URLDatasetPart.from_pandas(full_train_df)\n",
    "    full_eval_dataset = URLDatasetPart.from_pandas(full_eval_df)\n",
    "\n",
    "    cpu_eval_df = shorten_df_in_smart_way(full_eval_df, min(cpu_dataset_max_length, len(full_eval_df)), global_args.seed)\n",
    "    cpu_eval_dataset = URLDatasetPart.from_pandas(cpu_eval_df)\n",
    "\n",
    "\n",
    "    onnx_baseline_model_path = os.path.join(working_dir, 'onnx_baseline.onnx')\n",
    "    global_torch_model.create_onnx_file(onnx_baseline_model_path)\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"Quantization-just-unoptimized-float16-{model_type}-{global_args.dataset_name}\") as run:\n",
    "        print(run.info.run_id)\n",
    "        do_baseline_evaluation(onnx_baseline_model_path, cpu_eval_dataset=cpu_eval_dataset, full_eval_dataset=full_eval_dataset)\n",
    "        do_float16_part_without_optimizations(onnx_baseline_model_path, model_type=model_type, full_eval_dataset=full_eval_dataset)\n",
    "        do_optimized_model_part(onnx_baseline_model_path, full_eval_dataset=full_eval_dataset)\n",
    "        do_float16_part(onnx_baseline_model_path, model_type=model_type, full_eval_dataset=full_eval_dataset)\n",
    "        do_dynamic_quant(onnx_baseline_model_path, cpu_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60901cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_run_id = \"0997a077284848c094e5e6fa20e3f9a8\"\n",
    "global_torch_model = load_model_from_run(load_run_id)\n",
    "global_args = global_torch_model.args\n",
    "print(global_args)\n",
    "global_tokenizer = global_torch_model.tokenizer\n",
    "do_run(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfd8317",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_run_id = \"14aa0dcdb9344606a6b84778c9bcd69a\"\n",
    "global_torch_model = load_model_from_run(load_run_id)\n",
    "global_args = global_torch_model.args\n",
    "print(global_args)\n",
    "global_tokenizer = global_torch_model.tokenizer\n",
    "do_run(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f88c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_run_id = \"14aa0dcdb9344606a6b84778c9bcd69a\"\n",
    "global_torch_model = load_model_from_run(load_run_id)\n",
    "global_args = global_torch_model.args\n",
    "print(global_args)\n",
    "global_tokenizer = global_torch_model.tokenizer\n",
    "do_run(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175b4d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_run_id = \"94c1fa2098824e10be4fad32399fbc2b\"\n",
    "global_torch_model = load_model_from_run(load_run_id)\n",
    "global_args = global_torch_model.args\n",
    "print(global_args)\n",
    "global_tokenizer = global_torch_model.tokenizer\n",
    "do_run(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617ec980",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_run_id = \"11d9a273d9204e928736c5482ada67b2\"\n",
    "global_torch_model = load_model_from_run(load_run_id)\n",
    "global_args = global_torch_model.args\n",
    "print(global_args)\n",
    "global_tokenizer = global_torch_model.tokenizer\n",
    "do_run(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c480ca0",
   "metadata": {},
   "source": [
    "- how to load from filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fa0090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertForSequenceClassification, BertTokenizer\n",
    "# model_type = \n",
    "# local_output_dir = f'./myScriptsNotShare/joined_small_model'\n",
    "\n",
    "# # Model args\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--seed\", default=42)\n",
    "\n",
    "# parser.add_argument(\"--model_type\", default=f\"bert_small\", type=str, help=\"Type of BERT model to use. see 'get_model_checkpoint' method\")\n",
    "# parser.add_argument(\"--dropout\", default=0, type=float, help=\"Dropout rate on final classification layer\")\n",
    "# parser.add_argument(\"--max_sequence_length\", default=256)\n",
    "\n",
    "# # Training params\n",
    "# parser.add_argument(\"--batch_size\", default=128, type=int, help=\"Batch size\")\n",
    "# parser.add_argument(\"--epochs_max\", default=5, type=int, help=\"Maximum number of epochs. Can stop early, however\")\n",
    "# parser.add_argument(\"--patience\", default=3, type=int, help=\"Number of epochs to wait for validation accuracy increase before stopping. If it is set to None, then early stopping is not used\")\n",
    "# parser.add_argument(\"--freeze_epochs\", default=1, type=int, help=\"Number of epochs to freeze BERT non-final layers initially.\")\n",
    "# parser.add_argument(\"--loss\", default=\"focal\", choices=[\"cross_entropy\", \"focal\"], type=str, help=\"Loss function used\")\n",
    "# parser.add_argument(\"--focal_loss_gamma\", default=2, help=\"Pass gama parameter if focal loss is being used. Otherwise has no effect\")\n",
    "# parser.add_argument(\"--focal_loss_alpha\", default=-1, help=\"Pass alpha parameter if focal loss is being used. Otherwise has no effect\")\n",
    "# parser.add_argument(\"--decision_threshold\", default=0.5, type=float, help=\"If probability of a class 1 is higher than this, then the sample is classified as class 1\")\n",
    "# parser.add_argument(\"--weight_decay\", default=0.00, type=float, help=\"AdamW weight decay for both parts of the model\")\n",
    "# parser.add_argument(\"--bert_learning_rate\", default=1e-5, type=float, help=\"AdamW learning rate for everything in model except final classifaction layer\")\n",
    "# parser.add_argument(\"--classifier_learning_rate\", default=1e-5, type=float, help=\"AdamW learning rate for classification layer of model\")\n",
    "\n",
    "# # Dataset args\n",
    "# parser.add_argument(\"--dataset_name\", default=\"joined\", choices=[\"private_data\", \"any_public_dataset_name\"])\n",
    "# parser.add_argument(\"--train_folds\", default=None, type=str, help=\"Which folds of the dataset should be used for training\")\n",
    "# parser.add_argument(\"--eval_folds\", default=[4], type=str, help=\"Which folds of the dataset should be used for evaluation\")\n",
    "# parser.add_argument(\"--shorten_to_train\", default=None, help=\"How much should train dataset be shortened (400u - 400 records), (10% - 10 percent of all records)\")\n",
    "# parser.add_argument(\"--shorten_to_eval\", default=None, help=\"How much should test or validation set be shortened\")\n",
    "# parser.add_argument(\"--label_count\", default=2)\n",
    "\n",
    "\n",
    "# default_args = parser.parse_args([])\n",
    "# raw_model = BertForSequenceClassification.from_pretrained(local_output_dir)\n",
    "# tokenizer = BertTokenizer.from_pretrained(local_output_dir)\n",
    "# torch_model = TransformerModelManager(default_args, raw_model, tokenizer, None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
