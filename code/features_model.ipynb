{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "a27360c2-f292-494d-8ede-953a2cb0b394",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "import importlib\n",
                "import subprocess\n",
                "import sys\n",
                "from utils.environment_specific import is_local_development\n",
                "\n",
                "def install_if_missing(package_name, pip_name=None):\n",
                "    try:\n",
                "        importlib.import_module(package_name)\n",
                "    except ImportError:\n",
                "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pip_name or package_name])\n",
                "\n",
                "if not is_local_development():\n",
                "    install_if_missing(\"dotenv\", \"python-dotenv\")\n",
                "    install_if_missing(\"onnxruntime\")\n",
                "    install_if_missing(\"tldextract\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bf2f9a5c",
            "metadata": {},
            "outputs": [],
            "source": [
                "import time\n",
                "import random\n",
                "import os\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from dotenv import load_dotenv\n",
                "from sklearn.model_selection import RandomizedSearchCV, PredefinedSplit\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "import mlflow \n",
                "\n",
                "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
                "from utils.dataset import load_public_dataset, load_full_private_df, split_df_by_folds\n",
                "from utils.url_features import extract_all_vec\n",
                "from utils.base_models import find_decision_threshold_maximizing_f1, log_persistent_performance\n",
                "from utils.output import print_dict_level1_inline\n",
                "\n",
                "RANDOM_STATE = 42"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "cd4cb3de-3c7f-43ac-b92b-a3c8c3dc33d2",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "np.random.seed(RANDOM_STATE)\n",
                "# in case any standard library uses some random function\n",
                "random.seed(RANDOM_STATE)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "bf8eea3a-57c0-4e09-b55e-7765d9279657",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "loaded = load_dotenv(\".env\")\n",
                "if not loaded:\n",
                "    loaded = load_dotenv(\"../../.env\")\n",
                "assert loaded is True"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d9825ce7",
            "metadata": {},
            "outputs": [],
            "source": [
                "if is_local_development():\n",
                "    spark = None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "172d5af3",
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_dataset(name):\n",
                "    if name != \"private_data\":\n",
                "        df_train_urls, df_test_urls = load_public_dataset(name)\n",
                "    else:\n",
                "        df = load_full_private_df(spark)\n",
                "        df_train_urls, df_test_urls = split_df_by_folds(\n",
                "            df,\n",
                "            train_folds=None,\n",
                "            eval_folds=[4],\n",
                "            shorten_string_train=None,\n",
                "            shorten_string_eval=None,\n",
                "            seed=42,\n",
                "        )\n",
                "        \n",
                "\n",
                "    df_train_features = extract_all_vec(df_train_urls[\"url\"]).reset_index(drop=True)\n",
                "    df_test_features = extract_all_vec(df_test_urls[\"url\"]).reset_index(drop=True)\n",
                "\n",
                "    df_train = pd.concat([df_train_features, df_train_urls[[\"label\", \"fold\"]].reset_index(drop=True)], axis=1)\n",
                "    df_test = pd.concat([df_test_features, df_test_urls[[\"label\"]].reset_index(drop=True)], axis=1)\n",
                "\n",
                "    return {\"train\": df_train, \"test\": df_test}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "37e457d1",
            "metadata": {},
            "outputs": [],
            "source": [
                "datasets = {\n",
                "    \"grambeddings\": get_dataset(\"grambeddings\"),\n",
                "    \"kaggle_binary\": get_dataset(\"kaggle_binary\"),\n",
                "    \"kaggle_multiple\": get_dataset(\"kaggle_multiple\"),\n",
                "    \"mendeley\": get_dataset(\"mendeley\"),\n",
                "    \"joined\": get_dataset(\"mendeley\"),\n",
                "    \"private_data\": get_dataset(\"private_data\"),\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "960a0cf0-a3fa-461c-8d53-4a822dbfa052",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "def get_ready_dataset(name):\n",
                "    df_train, df_test = datasets[name][\"train\"], datasets[name][\"test\"]\n",
                "\n",
                "    assert \"label\" in df_test.columns, f\"Missing 'label' in test data for {name}\"\n",
                "\n",
                "    df_val = df_train[df_train[\"fold\"] == 3].reset_index(drop=True)\n",
                "    df_train = df_train[df_train[\"fold\"] != 3].reset_index(drop=True)\n",
                "\n",
                "    df_train = df_train.drop(columns=[\"fold\"])\n",
                "    df_val = df_val.drop(columns=[\"fold\"])\n",
                "    df_test = df_test.drop(columns=[\"fold\"], errors=\"ignore\")\n",
                "\n",
                "    X_train, y_train = df_train.drop(columns=[\"label\"]), df_train[\"label\"]\n",
                "    X_val, y_val = df_val.drop(columns=[\"label\"]), df_val[\"label\"]\n",
                "    X_test, y_test = df_test.drop(columns=[\"label\"]), df_test[\"label\"]\n",
                "\n",
                "    X_train_val = np.concatenate([X_train, X_val], axis=0)\n",
                "    y_train_val = np.concatenate([y_train, y_val], axis=0)\n",
                "\n",
                "    return {\n",
                "        \"X_train\": X_train,\n",
                "        \"y_train\": y_train,\n",
                "        \"X_val\": X_val,\n",
                "        \"y_val\": y_val,\n",
                "        \"X_test\": X_test,\n",
                "        \"y_test\": y_test,\n",
                "        \"X_train_val\": X_train_val,\n",
                "        \"y_train_val\": y_train_val,\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "4b64d751-cd36-4618-ae98-ef9f3dee0e6c",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "\n",
                "def scale_dataset(data: dict) -> dict:\n",
                "    X_train = data[\"X_train\"]\n",
                "    X_val = data[\"X_val\"]\n",
                "    X_test = data[\"X_test\"]\n",
                "    X_train_val = data[\"X_train_val\"]\n",
                "    y_train = data[\"y_train\"]\n",
                "    y_val = data[\"y_val\"]\n",
                "    y_test = data[\"y_test\"]\n",
                "    y_train_val = data[\"y_train_val\"]\n",
                "\n",
                "    # Scaler for train/val (used during model selection)\n",
                "    scaler_train_val = StandardScaler()\n",
                "    X_train_scaled = scaler_train_val.fit_transform(X_train)\n",
                "    X_val_scaled = scaler_train_val.transform(X_val)\n",
                "\n",
                "    # Scaler for full training (train + val) used for final model\n",
                "    scaler_whole = StandardScaler()\n",
                "    X_train_val_scaled = scaler_whole.fit_transform(X_train_val)\n",
                "    X_test_scaled = scaler_whole.transform(X_test)\n",
                "\n",
                "    return {\n",
                "        \"X_train\": X_train_scaled,\n",
                "        \"y_train\": y_train,\n",
                "        \"X_val\": X_val_scaled,\n",
                "        \"y_val\": y_val,\n",
                "        \"X_test\": X_test_scaled,\n",
                "        \"y_test\": y_test,\n",
                "        \"X_train_val\": X_train_val_scaled,\n",
                "        \"y_train_val\": y_train_val,\n",
                "        \"scaler_train_val\": scaler_train_val,\n",
                "        \"scaler_whole\": scaler_whole,\n",
                "    }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "acd9ed4c-3ef8-4fcb-a59f-9cb358ee427d",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "source": [
                "# Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "d80bc1cc-2f0b-4bfb-8886-b85e96b8eb9d",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "def calculate_metrics(\n",
                "    true_labels,\n",
                "    probabilities,\n",
                "    predictions,\n",
                "):\n",
                "    \"\"\"\n",
                "    true_labels   : 1-D array-like\n",
                "    probabilities : array-like of shape (n_samples, n_classes)\n",
                "    predictions   : 1-D array-like\n",
                "    \"\"\"\n",
                "    true_labels = np.asarray(true_labels)\n",
                "    probabilities = np.asarray(probabilities)\n",
                "    predictions = np.asarray(predictions, dtype=int)\n",
                "\n",
                "    if not (len(true_labels) == len(probabilities) == len(predictions)):\n",
                "        raise ValueError(\"true_labels, probabilities and predictions must have equal length\")\n",
                "\n",
                "    n_classes = probabilities.shape[1]\n",
                "\n",
                "    metrics = classification_report(true_labels, predictions, output_dict=True, zero_division=0)\n",
                "    cm = confusion_matrix(true_labels, predictions)\n",
                "    metrics[\"confusion_matrix\"] = cm.tolist()\n",
                "\n",
                "    if n_classes == 2:\n",
                "        metrics[\"roc_auc_score\"] = roc_auc_score(true_labels, probabilities[:, 1])\n",
                "\n",
                "    return metrics\n",
                "\n",
                "\n",
                "def predict(class_probs):\n",
                "    assert class_probs.shape[1] >= 2\n",
                "    return np.argmax(class_probs, axis=1)\n",
                "\n",
                "\n",
                "def predict_with_threshold(class_probs, threshold):\n",
                "    assert class_probs.shape[1] == 2\n",
                "    return (class_probs[:, 1] >= threshold).astype(int)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "d3aa6032-34af-45eb-96b6-e19190a86ac2",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "def _as_numpy(X):\n",
                "    \"\"\"Safely convert pandas or NumPy to ndarray for concatenation.\"\"\"\n",
                "    return X.values if isinstance(X, (pd.DataFrame, pd.Series)) else np.asarray(X)\n",
                "\n",
                "\n",
                "def run_experiment(\n",
                "    data: dict,\n",
                "    model,\n",
                "    param_distributions: dict,\n",
                "    *,\n",
                "    n_iter: int = 30,\n",
                "    scoring: str = \"f1_macro\",\n",
                "    random_state: int = 42,\n",
                "    path_to_log\n",
                "    # n_jobs: int = -1,\n",
                "):\n",
                "    X_train, y_train = _as_numpy(data[\"X_train\"]), _as_numpy(data[\"y_train\"])\n",
                "    X_val, y_val = _as_numpy(data[\"X_val\"]), _as_numpy(data[\"y_val\"])\n",
                "    X_test, y_test = _as_numpy(data[\"X_test\"]), _as_numpy(data[\"y_test\"])\n",
                "    X_train_val, y_train_val = _as_numpy(data[\"X_train_val\"]), _as_numpy(data[\"y_train_val\"])\n",
                "\n",
                "    X_search = np.concatenate([X_train, X_val], axis=0)\n",
                "    y_search = np.concatenate([y_train, y_val], axis=0)\n",
                "    test_fold = np.concatenate([np.full(len(X_train), -1), np.zeros(len(X_val))])\n",
                "    cv = PredefinedSplit(test_fold)\n",
                "\n",
                "    search = RandomizedSearchCV(\n",
                "        estimator=model,\n",
                "        param_distributions=param_distributions,\n",
                "        n_iter=n_iter,\n",
                "        scoring=scoring,\n",
                "        cv=cv,\n",
                "        # n_jobs=n_jobs,\n",
                "        random_state=random_state,\n",
                "        verbose=1,\n",
                "    )\n",
                "    search.fit(X_search, y_search)\n",
                "    print(f\"\\nBest val {scoring}: {search.best_score_:.4f}\")\n",
                "    print(\"Best params:\", search.best_params_)\n",
                "\n",
                "    best_model = search.best_estimator_\n",
                "    best_model.fit(X_train_val, y_train_val)\n",
                "\n",
                "    # evaluate on test\n",
                "    y_probs_test = best_model.predict_proba(X_test)\n",
                "    predictions = predict(y_probs_test)\n",
                "    metrics = calculate_metrics(\n",
                "        true_labels=y_test,\n",
                "        probabilities=y_probs_test,\n",
                "        predictions=predictions,\n",
                "    )\n",
                "\n",
                "    if y_probs_test.shape[1] == 2:\n",
                "        class_1_probs = y_probs_test[:, 1]\n",
                "        best_threshold = find_decision_threshold_maximizing_f1(probs=class_1_probs, labels=y_test)\n",
                "        metrics_best = calculate_metrics(\n",
                "            true_labels=y_test,\n",
                "            probabilities=y_probs_test,\n",
                "            predictions=predict_with_threshold(y_probs_test, best_threshold),\n",
                "        )\n",
                "        metrics_best[\"best_threshold\"] = best_threshold\n",
                "        prefix_path = f'{path_to_log}/'\n",
                "        log_persistent_performance(metrics=metrics, best_threshold_metrics=metrics_best, true_labels=y_test, class_probabilities=y_probs_test, predictions=predictions, store_predictions=False, prefix=prefix_path)\n",
                "    else:\n",
                "        metrics_best = metrics\n",
                "        metrics_best[\"best_threshold\"] = None\n",
                "        mlflow.log_dict(metrics, artifact_file=f\"{path_to_log}/best_model_metrics.json\")\n",
                "        \n",
                "    print_dict_level1_inline(metrics)\n",
                "\n",
                "    # inference speed\n",
                "    t0 = time.perf_counter()\n",
                "    _ = best_model.predict(X_test)\n",
                "    t_elapsed = time.perf_counter() - t0\n",
                "    sps = len(X_test) / t_elapsed\n",
                "    sps_inv = t_elapsed / len(X_test)\n",
                "    print(\"\\nInference speed\")\n",
                "    print(f\"total time (s): {t_elapsed:.4f}\")\n",
                "    print(f\"samples / sec : {sps:.2f}\")\n",
                "    print(f\"sec / sample  : {sps_inv:.6f}\")\n",
                "\n",
                "    # pack results\n",
                "    perf_score = {\"n_per_s\": sps, \"s_per_1\": sps_inv}\n",
                "    to_store = {\n",
                "        \"best_val_score\": search.best_score_,\n",
                "        \"best_params\": search.best_params_,\n",
                "        \"test_metrics\": metrics,\n",
                "        \"perf_score\": perf_score,\n",
                "    }\n",
                "    return to_store, best_model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "97a292b1-644e-4807-a214-00f42536f02a",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "def store_sorted_feature_importance_dict(feature_importance_dict: dict, artifact_file: str):\n",
                "    sorted_importance = sorted(\n",
                "        feature_importance_dict.items(),\n",
                "        key=lambda x: x[1],\n",
                "        reverse=True,\n",
                "    )\n",
                "    sorted_dict = [{\"feature\": k, \"importance\": float(v)} for k, v in sorted_importance]\n",
                "    mlflow.log_dict({\"feature_importance\": sorted_dict}, artifact_file=artifact_file)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "3db377d6-deec-4f91-91b4-66a4fef2637f",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "def get_path_to_log(model_name, dataset_name):\n",
                "    print(f\"=== {dataset_name} ===\")\n",
                "    path = os.path.join(model_name, dataset_name)\n",
                "    os.makedirs(path, exist_ok=True)\n",
                "    return path"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "d8d29f28-5c76-4a63-b1b3-cc9c9e715a43",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "client = mlflow.MlflowClient()\n",
                "experiments_folder_path = os.getenv(\"EXPERIMENTS_PATH\")\n",
                "if is_local_development():\n",
                "    experiment_name = \"feature_models\"\n",
                "    print()\n",
                "else:\n",
                "    experiment_name = os.getenv(\"EXPERIMENT_NAME\")\n",
                "\n",
                "experiment_path = os.path.join(experiments_folder_path, experiment_name)\n",
                "mlflow.set_experiment(experiment_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "b5429798-1c23-4003-9436-bd78a78ef5e8",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "run = mlflow.start_run(run_name=\"Feature based models\")\n",
                "print(run.info.run_id)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "160456aa-180c-4323-9eba-991c65be4fd9",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "source": [
                "## Logistic regression"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "c30d8de8-edaa-4fb6-aab4-2fce88f4b1ce",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "logistic_regression_param_space = {\n",
                "    # inverse strength of regularization\n",
                "    \"C\": [0.001, 0.01, 0.1, 1.0, 10.0, 100.0],\n",
                "    \"penalty\": [\"l2\"],\n",
                "    \"solver\": [\"lbfgs\"],\n",
                "    \"max_iter\": [300, 800],\n",
                "}\n",
                "\n",
                "for ds_name in datasets.keys():\n",
                "    path = get_path_to_log(\"logistic_regression\", ds_name)\n",
                "\n",
                "    unscaled_dataset = get_ready_dataset(ds_name)\n",
                "    dataset = scale_dataset(unscaled_dataset)\n",
                "    to_store, model = run_experiment(dataset, LogisticRegression(max_iter=1000, n_jobs=-1, class_weight=\"balanced\"), logistic_regression_param_space, n_iter=3, path_to_log=path)\n",
                "\n",
                "    feature_names = list(unscaled_dataset[\"X_train\"].columns)\n",
                "    coef = model.coef_\n",
                "    if coef.ndim == 1:\n",
                "        values = np.abs(coef)\n",
                "    else:\n",
                "        values = np.mean(np.abs(coef), axis=0)\n",
                "\n",
                "    feature_importance_dict = dict(zip(feature_names, values))\n",
                "    store_sorted_feature_importance_dict(\n",
                "        feature_importance_dict,\n",
                "        artifact_file=f\"{path}/feature_importance.json\"\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "dd69bc06-6884-41c9-b44d-41f3aadb6730",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "source": [
                "## Naive Bayes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "19167df9-60db-4a35-b1a8-2f2bcd3752d8",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "from sklearn.naive_bayes import GaussianNB\n",
                "\n",
                "naive_bayes_param_space = {\n",
                "    \"var_smoothing\": [1e-11, 1e-9, 1e-7],\n",
                "}\n",
                "\n",
                "for ds_name in datasets.keys():\n",
                "    path = get_path_to_log(\"naive_bayes\", ds_name)\n",
                "\n",
                "    dataset = get_ready_dataset(ds_name)\n",
                "\n",
                "    to_store, model = run_experiment(\n",
                "        dataset,\n",
                "        GaussianNB(),\n",
                "        naive_bayes_param_space,\n",
                "        n_iter=3,\n",
                "        path_to_log=path\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "c5be99aa-c628-4286-9c7a-5adc1024985f",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "source": [
                "## XGBoost"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "d41372b8-978e-4c3e-aad8-51ef095df143",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "from xgboost import XGBClassifier\n",
                "\n",
                "xgboost_param_space = {\n",
                "    'n_estimators': [100, 200, 300],\n",
                "    'max_depth': [3, 5, 7, 9],\n",
                "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
                "    'subsample': [0.6, 0.8, 1.0],\n",
                "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
                "    'gamma': [0, 1, 5],\n",
                "    'min_child_weight': [1, 3, 5]\n",
                "}\n",
                "\n",
                "for ds_name in datasets.keys():\n",
                "    path = get_path_to_log(\"xgboost\", ds_name)\n",
                "\n",
                "    dataset = get_ready_dataset(ds_name)\n",
                "\n",
                "    y_train = dataset[\"y_train\"]\n",
                "    classes = np.unique(y_train)\n",
                "    is_binary = len(classes) == 2\n",
                "\n",
                "    if is_binary:\n",
                "        pos = np.sum(y_train == 1)\n",
                "        neg = np.sum(y_train == 0)\n",
                "        scale_pos_weight = neg / pos if pos > 0 else 1.0\n",
                "        extra = {\n",
                "            \"objective\": \"binary:logistic\",\n",
                "            \"scale_pos_weight\": scale_pos_weight,\n",
                "        }\n",
                "    else:\n",
                "        extra = {\n",
                "            \"objective\": \"multi:softprob\",\n",
                "            \"num_class\": len(classes),\n",
                "        }\n",
                "\n",
                "    to_store, model = run_experiment(\n",
                "        dataset,\n",
                "        XGBClassifier(\n",
                "            tree_method=\"hist\",\n",
                "            # use_label_encoder=False,\n",
                "            eval_metric=\"logloss\",\n",
                "            **extra,\n",
                "        ),\n",
                "        xgboost_param_space,\n",
                "        n_iter=25,\n",
                "        path_to_log=path,\n",
                "    )\n",
                "\n",
                "    feature_names = list(dataset[\"X_train\"].columns)\n",
                "    feature_importance_dict = dict(zip(feature_names, model.feature_importances_))\n",
                "\n",
                "    store_sorted_feature_importance_dict(\n",
                "        feature_importance_dict,\n",
                "        artifact_file=f\"{path}/feature_importance.json\"\n",
                "    )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "8ac98aa7-e68a-40d0-991e-d7152881b6ef",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "mlflow.end_run()"
            ]
        }
    ],
    "metadata": {
        "application/vnd.databricks.v1+notebook": {
            "computePreferences": null,
            "dashboards": [],
            "environmentMetadata": null,
            "inputWidgetPreferences": null,
            "language": "python",
            "notebookMetadata": {
                "pythonIndentUnit": 4
            },
            "notebookName": "features_model",
            "widgets": {}
        },
        "kernelspec": {
            "display_name": "bp-env-cpu",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
